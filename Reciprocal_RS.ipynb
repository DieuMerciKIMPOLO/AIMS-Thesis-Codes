{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/temci/Documents/Thesis_aims/Data\n"
     ]
    }
   ],
   "source": [
    "cd Documents/Thesis_aims/Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### Loading Data ######################################################################\n",
    "\n",
    "job_data = pd.read_table('Bons/total_jobs.csv')#, sep = r'\\t', engine = 'python') #, nrows = 1000)\n",
    "users_data = pd.read_table('Bons/total_users.csv')#, sep = '\\t')\n",
    "users_hist_data = pd.read_table('Bons/users_past_jobs.csv', sep = '\\t')\n",
    "apps_train_data = pd.read_table('Bons/applications_train.csv')#, sep = '\\t')#[['WindowID'] == 1]\n",
    "apps_test_data = pd.read_table('Bons/applications_test.csv')\n",
    "interv1 = pd.read_table('Bons/positive_replies.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "########################## Removing HTML markups#########################################################\n",
    "\n",
    "def clean_html(html):\n",
    "    \"\"\"\n",
    "    Copied from NLTK package.\n",
    "    Remove HTML markup from the given string.\n",
    "\n",
    "    :param html: the HTML string to be cleaned\n",
    "    :type html: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    # First we remove inline JavaScript/CSS:\n",
    "    cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", str(html).strip())\n",
    "    # Then we remove html comments. This has to be done before removing regular\n",
    "    # tags since comments can contain '>' characters.\n",
    "    cleaned = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned)\n",
    "    # Next we can remove the remaining tags:\n",
    "    cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
    "    # Finally, we deal with whitespace\n",
    "    cleaned = re.sub(r\"&nbsp;\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    cleaned = re.sub('\\r', ' ', cleaned)\n",
    "    cleaned = re.sub('/', ' ', cleaned)\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "############################### Preprecessing data ##################################################\n",
    "\n",
    "user_defined_stop_words = ['r'] \n",
    "i = nltk.corpus.stopwords.words('english')\n",
    "j = list(string.punctuation) + user_defined_stop_words\n",
    "stopwords = set(i).union(j)\n",
    "def preprocess(x):\n",
    "    x = re.sub('<b>', ' ', str(x))\n",
    "    x = re.sub('</b>', ' ', x.lower())\n",
    "    x = re.sub('<p>', ' ', x.lower())\n",
    "    x = re.sub('</p>', ' ', x.lower())\n",
    "    x = re.sub('</span>', ' ', x.lower())\n",
    "    x = re.sub('<span>', ' ', x.lower())\n",
    "    x = re.sub('[^a-z\\s]', ' ', x.lower())                  # get rid of noise\n",
    "    x = re.sub(r'  ', ' ', x.lower())\n",
    "    x = re.sub(r'   ', ' ', x.lower())\n",
    "    x = [w for w in x.split() if w not in set(stopwords)]  # remove stopwords\n",
    "    return ' '.join(x)                                     # join the list\n",
    "corpus = job_data['Title'].append(users_hist_data['JobTitle'])\n",
    "descriptions = corpus.apply(clean_html) \n",
    "descriptions = descriptions.apply(preprocess)#job_data['Description'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "########################### the content-based recommender system ############################################\n",
    "\n",
    "ds = descriptions.dropna()\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=1, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(ds.dropna())\n",
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "\n",
    "diction = {}\n",
    "i = 0\n",
    "for idx1, row in ds.iteritems():\n",
    "    diction[i] = idx1\n",
    "    i += 1\n",
    "\n",
    "\n",
    "idx = 0\n",
    "# for idx, row in ds.iteritems():\n",
    "similar_all_items = pd.DataFrame(columns = ['JobID','UserID','Similarity'])\n",
    "while idx < 16627 : #13084 :#16855 : \n",
    "    similar_items = pd.DataFrame()\n",
    "#     similar_indices = cosine_similarities[idx][13084:].argsort()[:-21:-1]\n",
    "    for i in cosine_similarities[idx][16627:].argsort()[:-21:-1]:#similar_indices :\n",
    "        similar_items['JobID'] = [job_data.loc[diction[idx]].JobID]#:, 'JobID']\n",
    "        similar_items['UserID'] = [users_hist_data.loc[diction[i+16627]].UserID]#:, 'UserID']\n",
    "        similar_items['Similarity'] = [cosine_similarities[idx][i+16627]]\n",
    "        similar_all_items = similar_all_items.append(similar_items, ignore_index = True)\n",
    "#     similar_items = [(cosine_similarities[idx][i+13084], (dict[idx],dict[i+13084])) for i in similar_indices]\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "########################### Popularity-based, collaborative filtering and adding reciprocity ############################\n",
    "\n",
    "reduc_users = np.unique(users_data['UserID'])\n",
    "merge_all_job = pd.DataFrame()\n",
    "merge_ = pd.DataFrame()\n",
    "pop = pd.DataFrame()\n",
    "popular = apps_train_data.groupby(['JobID'], as_index = False).count()\\\n",
    "    .rename(columns = {'UserID':'Count'}).sort_values(['Count'], ascending=False)[['JobID','Count']]\n",
    "\n",
    "for user in reduc_users :\n",
    "#     app_user = apps_data[apps_data['UserID'] == user ]\n",
    "#     app_user = jobs[jobs['UserID'] == user]\n",
    "    app_user = apps_train_data[apps_train_data['UserID'] == user]\n",
    "    job_user = np.unique(app_user['JobID'])\n",
    "#     sim_user = jobs[jobs['JobID'].isin(job_user)]\n",
    "    popul = popular[~popular['JobID'].isin(job_user)][0:5]\n",
    "    popul['UserID'] = user\n",
    "    pop = pop.append(popul, ignore_index = True)\n",
    "    \n",
    "    sim_user = apps_train_data[apps_train_data['JobID'].isin(job_user)]\n",
    "    sim_user = sim_user[sim_user['UserID'] != user]\n",
    "    sim_user = sim_user.groupby(['UserID'], as_index = False).count()\\\n",
    "       .rename(columns = {'JobID':'Count'}).sort_values(['Count'], ascending=False)[['UserID', 'Count']][:10]\n",
    "#     sim_unique_user = np.unique(sim_user['UserID'])\n",
    "#     us_job = jobs[jobs['UserID'].isin(sim_unique_user)]\n",
    "#     us_job = jobs[jobs['UserID'].isin(sim_user['UserID'])]\n",
    "    us_job = apps_train_data[apps_train_data['UserID'].isin(sim_user['UserID'])]\n",
    "    us_job = us_job[~us_job['JobID'].isin(job_user)]\n",
    "    us_job = us_job.groupby(['JobID'],as_index=False).count().rename(columns = {'UserID':'Count'})\\\n",
    "          .sort_values(['Count'], ascending=False)[['JobID','Count']]\n",
    "#     us_job['UserId'] = user\n",
    "#     us_job = us_job.rename(columns = {'JobID':'JobId'})\n",
    "    merge_all_job = merge_all_job.append(us_job[0:40], ignore_index = True)\n",
    "    test = similar_all_items[similar_all_items['UserID'] == user]#.sort_values(['Similarity'], ascending=False)\n",
    "    us_job1 = us_job.merge(test, on = 'JobID')[0:40]\n",
    "    if not us_job1.empty :\n",
    "        us_job1['UserID'] = us_job1['UserID_y']\n",
    "        us_job1 = us_job1[['Count','JobID','Similarity','UserID']]\n",
    "        merge_ = merge_.append(us_job1, ignore_index = True)\n",
    "    else :\n",
    "        us_job['Similarity'] = 0\n",
    "        merge_ = merge_.append(us_job, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "######################## Recall of the users RS ############################################################ \n",
    "\n",
    "from __future__ import division\n",
    "distinct = np.unique(merge_all_job['UserID'])\n",
    "i = 0\n",
    "evaluation = pd.DataFrame()\n",
    "for user in distinct :\n",
    "    testt = pd.DataFrame()\n",
    "    user_chosen= merge_all_job[merge_all_job['UserID'] == user][['UserID','JobID']]\n",
    "    vue = apps_test_data[apps_test_data['UserID'] == user][['JobID']]\n",
    "    vu = user_chosen.merge(vue, on = 'JobID')\n",
    "    if len(vue) != 0 and len(user_chosen) != 0:\n",
    "        testt['Recall'] = [len(vu)/len(vue)]\n",
    "        testt['UserID'] = [user]\n",
    "        evaluation = evaluation.append(testt, ignore_index = True).sort_values(['Recall'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "############################### Recall of the reciprocal RS #############################################\n",
    "\n",
    "from __future__ import division\n",
    "distinct = np.unique(merge_all_job['UserID'])\n",
    "i = 0\n",
    "evaluation1 = pd.DataFrame()\n",
    "for user in distinct :\n",
    "    testt = pd.DataFrame()\n",
    "    user_chosen= merge_[merge_['UserID'] == user][['UserID','JobID']]\n",
    "    vue = apps_test_data[apps_test_data['UserID'] == user][['JobID']]\n",
    "    vu = user_chosen.merge(vue, on = 'JobID')\n",
    "    if len(vue) != 0 and len(user_chosen) != 0:\n",
    "        testt['Recall'] = [len(vu)/len(vue)]\n",
    "        testt['UserID'] = [user]\n",
    "        evaluation1 = evaluation1.append(testt, ignore_index = True).sort_values(['Recall'], ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "################## Recall of the popularity-based RS #######################################################\n",
    "\n",
    "from __future__ import division\n",
    "distinct = np.unique(pop['UserID'])\n",
    "i = 0\n",
    "evaluation2 = pd.DataFrame()\n",
    "for user in distinct :\n",
    "    testt = pd.DataFrame()\n",
    "    user_chosen= pop[pop['UserID'] == user][['UserID','JobID']]\n",
    "    vue = apps_test_data[apps_test_data['UserID'] == user][['JobID']]\n",
    "    vu = user_chosen.merge(vue, on = 'JobID')\n",
    "    if len(vue) != 0 and len(user_chosen) != 0:\n",
    "        testt['Recall'] = [len(vu)/len(vue)]\n",
    "        testt['UserID'] = [user]\n",
    "        evaluation2 = evaluation2.append(testt, ignore_index = True).sort_values(['Recall'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "############################### Precision of the reciprocal RS precision #############################\n",
    "\n",
    "from __future__ import division\n",
    "distinct = np.unique(merge_['UserID'])\n",
    "i = 0\n",
    "evaluation3 = pd.DataFrame()\n",
    "for user in distinct :\n",
    "    testt = pd.DataFrame()\n",
    "    user_chosen= merge_[merge_['UserID'] == user][['UserID','JobID']]\n",
    "    vue = apps_test_data[apps_test_data['UserID'] == user][['JobID']]\n",
    "    vu = user_chosen.merge(vue, on = 'JobID')\n",
    "    interviewed = vu.merge(interv1, on = ['UserID','JobID'])\n",
    "    total_interviewed = interv1[interv1['UserID'] == user]\n",
    "    if len(vu)!=0 and len(total_interviewed)!= 0 :\n",
    "        listee = [] \n",
    "        listee.append(len(interviewed)/len(vu))\n",
    "        testt['Precision'] = listee \n",
    "        testt['UserID'] = [user]\n",
    "        evaluation3 = evaluation3.append(testt, ignore_index = True).sort_values(['Recall'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "############################# Precision of the users RS precision #############################\n",
    "\n",
    "from __future__ import division\n",
    "distinct = np.unique(merge_all_job['UserID'])\n",
    "i = 0\n",
    "evaluation4 = pd.DataFrame()\n",
    "for user in distinct :\n",
    "    testt = pd.DataFrame()\n",
    "    user_chosen= merge_all_job[merge_all_job['UserID'] == user][['UserID','JobID']]\n",
    "    vue = apps_test_data[apps_test_data['UserID'] == user][['JobID']]\n",
    "    vu = user_chosen.merge(vue, on = 'JobID')\n",
    "    interviewed = vu.merge(interv1, on = ['UserID','JobID'])\n",
    "    total_interviewed = interv1[interv1['UserID'] == user]\n",
    "    if len(vu)!=0 and len(total_interviewed)!= 0 :\n",
    "        listee = [] \n",
    "        listee.append(len(interviewed)/len(vu))\n",
    "        testt['Precision'] = listee \n",
    "        testt['UserID'] = [user]\n",
    "        evaluation4 = evaluation4.append(testt, ignore_index = True).sort_values(['Recall'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################### Mean of the recommendations precisions ################################\n",
    "\n",
    "# for users RS\n",
    "print(evaluation4['Precision'].mean())\n",
    "\n",
    "#for reciprocal RS\n",
    "print(evaluation3['Precision'].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################### Mean of the recommendations recalls ################################\n",
    "\n",
    "# for users RS\n",
    "print(evaluation['Recall'].mean())\n",
    "\n",
    "#for reciprocal RS\n",
    "print(evaluation1['Recall'].mean())\n",
    "\n",
    "# for popularity-based RS\n",
    "print(evaluation2['Recall'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## Plotting to compare popularity-based,users and reciprocal RS ###################################\n",
    "\n",
    "\n",
    "#The data\n",
    "recall_10 = (0.4429, 0.4455, 0.4459, 0.4460, 0.4460)\n",
    "precision_popular = (0.0324, 0.0270,0.0260, 0.0237,0.0216)\n",
    "recall_popular = (0.0063, 0.0104, 0.0199, 0.0272,0.0331)\n",
    "recall_simple = (0.2029, 0.3203, 0.4226, 0.4779,0.5190)\n",
    "indices = [5,10,20,30,40]#,50,60]\n",
    "#Calculate optimal width\n",
    "width = np.min(np.diff(indices))/4\n",
    "\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "ax1.bar(indices-width,recall_10,width,color='b',label='Reciprocal RS')\n",
    "ax1.bar(indices,recall_popular,width,color='y',label='Popularity-based RS')\n",
    "ax1.bar(indices+width,recall_simple,width,color='r',label='Popularity-based RS')\n",
    "ax1.set_xlabel('Top-N')\n",
    "ax1.set_ylabel('Recall')\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################# Plotting to compare users (traditional) and reciprocal RS #########################################\n",
    "\n",
    "\n",
    "#The data\n",
    "precision_10 = (0.2564, 0.2612, 0.2615, 0.2615, 0.2615)\n",
    "precision_simple = (0.0230, 0.0219, 0.0205, 0.0202, 0.0204)\n",
    "indices = [5,10,20,30,40]\n",
    "#Calculate optimal width\n",
    "width = np.min(np.diff(indices))/3\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(indices-width,precision_10,width,color='b',label='Reciprocal RS')\n",
    "ax.bar(indices,precision_simple,width,color='r',label='User RS')\n",
    "ax.set_xlabel('Top-N')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
